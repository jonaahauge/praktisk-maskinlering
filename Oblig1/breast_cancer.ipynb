{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd3f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73dedc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Oblig1/wdbc.data\", header=None)\n",
    "df[\"target\"] = np.where(df[1] == \"M\", 1, 0)\n",
    "df = df.drop([0,1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a790f4",
   "metadata": {},
   "source": [
    "Først leser vi inn datasetet og ser hvilke verdier vi h. I og konvertere verdiene i target kollonen til 1 for M og 0 for B, slik at disse er på et format som kan brukes senere, vi legger også disse verdiene i en ny kollone slik at vi kan bruke disse  Vi tar også og droppe kollonen for ID-veriden og den gamle kollonen med target verdiene\n",
    "\n",
    "\n",
    "\n",
    "ref for np.where: https://www.geeksforgeeks.org/numpy/numpy-where-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "15413364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    train_val = data.groupby(\"target\", group_keys=False)[data.columns].sample(frac=0.8)\n",
    "    test = data.drop(train_val.index)\n",
    "    train = train_val.groupby(\"target\", group_keys=False)[train_val.columns].sample(frac=0.75)\n",
    "    validation = train_val.drop(train.index)    \n",
    "    return [train, validation, test]\n",
    "\n",
    "def check_split_dataset(data):\n",
    "    print(data[0][\"target\"].value_counts()/data[0].shape[0])\n",
    "    print(data[1][\"target\"].value_counts()/data[1].shape[0])\n",
    "    print(data[2][\"target\"].value_counts()/data[2].shape[0])\n",
    "\n",
    "def split_targets_and_features(data):\n",
    "    targets = data.target\n",
    "    features = data.drop(\"target\", axis=1)\n",
    "    return [targets, features]\n",
    "\n",
    "def create_set_of_targets_and_features(data):\n",
    "    train = split_targets_and_features(data[0])\n",
    "    validation = split_targets_and_features(data[1])\n",
    "    test = split_targets_and_features(data[2])\n",
    "\n",
    "    return [train, validation, test]\n",
    "\n",
    "def create_full_dataset(input):\n",
    "    data = split_dataset(input)\n",
    "    dataset = create_set_of_targets_and_features(data)\n",
    "\n",
    "    return dataset\n",
    "        \n",
    "def evaluate_model(classifier, features, targets):\n",
    "    predictions = classifier.predict(features)\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    precision = precision_score(targets, predictions)\n",
    "    recall = recall_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    roc_auc = roc_auc_score(targets, predictions)   \n",
    "\n",
    "    return [accuracy, precision, recall, f1, roc_auc]\n",
    "\n",
    "def multiple_evaluate_model(classifier, features, targets):\n",
    "    accuracy, precision, recall, f1, roc_auc = [], [], [], [], []\n",
    "\n",
    "    for i in range(0,30,1):\n",
    "        result = evaluate_model(classifier, features, targets)\n",
    "        accuracy.append(result[0])\n",
    "        precision.append(result[1])\n",
    "        recall.append(result[2])\n",
    "        f1.append(result[3])\n",
    "        roc_auc.append(result[4])\n",
    "\n",
    "    print(len(accuracy))\n",
    "    print(f'Accuracy: {np.mean(accuracy):.2f}')\n",
    "    print(f'Precision: {np.mean(precision):.2f}')\n",
    "    print(f'Recall: {np.mean(recall):.2f}')\n",
    "    print(f'F1: {np.mean(f1):.2f}')\n",
    "    print(f'ROCAUC: {np.mean(roc_auc):.2f}')\n",
    "\n",
    "def append_values(results, accuracy, precision, recall, f1, roc_auc):\n",
    "        accuracy.append(results[0])\n",
    "        precision.append(results[1])\n",
    "        recall.append(results[2])\n",
    "        f1.append(results[3])\n",
    "        roc_auc.append(results[4])\n",
    "\n",
    "def results_print(accuracy, precision, recall, f1, roc_auc):\n",
    "    print(f'Accuracy: {np.mean(accuracy):.2f}')\n",
    "    print(f'Precision: {np.mean(precision):.2f}')\n",
    "    print(f'Recall: {np.mean(recall):.2f}')\n",
    "    print(f'F1: {np.mean(f1):.2f}')\n",
    "    print(f'ROCAUC: {np.mean(roc_auc):.2f}')\n",
    "\n",
    "def test_results_print(accuracy, precision, recall, f1, roc_auc):\n",
    "    print(f'Accuracy: {np.mean(accuracy):.2f} +- {np.std(accuracy):.2f}')\n",
    "    print(f'Precision: {np.mean(precision):.2f} +- {np.std(precision):.2f}')\n",
    "    print(f'Recall: {np.mean(recall):.2f} +- {np.std(recall):.2f}')\n",
    "    print(f'F1: {np.mean(f1):.2f} +- {np.std(f1):.2f}')\n",
    "    print(f'ROCAUC: {np.mean(roc_auc):.2f} +- {np.std(roc_auc):.2f}')\n",
    "\n",
    "def validate(classifier):    \n",
    "    train_accuracy, train_precision, train_recall, train_f1, train_roc_auc = [], [], [], [], []\n",
    "    validate_accuracy, validate_precision, validate_recall, validate_f1, validate_roc_auc = [], [], [], [], []\n",
    "\n",
    "    for i in range(0, 10, 1):\n",
    "        dataset = create_full_dataset(df)\n",
    "        classifier.fit(dataset[0][1], dataset[0][0])\n",
    "    \n",
    "        results_train = evaluate_model(classifier, dataset[0][1], dataset[0][0])\n",
    "        append_values(results_train, train_accuracy, train_precision, train_recall, train_f1, train_roc_auc)\n",
    "    \n",
    "        results_validate = evaluate_model(classifier, dataset[1][1], dataset[1][0])\n",
    "        append_values(results_validate, validate_accuracy, validate_precision, validate_recall, validate_f1, validate_roc_auc)\n",
    "\n",
    "    print('Training results:')\n",
    "    results_print(train_accuracy, train_precision, train_recall, train_f1, train_roc_auc)\n",
    "    print('Validation results:')\n",
    "    results_print(validate_accuracy, validate_precision, validate_recall, validate_f1, validate_roc_auc)\n",
    "\n",
    "def test(classifier):\n",
    "    accuracy, precision, recall, f1, roc_auc = [], [], [], [], []\n",
    "\n",
    "    for i in range(0, 10, 1):\n",
    "        dataset = create_full_dataset(df)\n",
    "        classifier.fit(dataset[0][1], dataset[0][0])\n",
    "\n",
    "        results = evaluate_model(classifier, dataset[2][1], dataset[2][0])\n",
    "        append_values(results, accuracy, precision, recall, f1, roc_auc)\n",
    "    \n",
    "    print('Test results:')\n",
    "    test_results_print(accuracy, precision, recall, f1, roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "74c20907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results:\n",
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1: 1.00\n",
      "ROCAUC: 1.00\n",
      "Validation results:\n",
      "Accuracy: 0.93\n",
      "Precision: 0.91\n",
      "Recall: 0.90\n",
      "F1: 0.90\n",
      "ROCAUC: 0.92\n",
      "Test results:\n",
      "Accuracy: 0.92 +- 0.03\n",
      "Precision: 0.90 +- 0.04\n",
      "Recall: 0.90 +- 0.04\n",
      "F1: 0.90 +- 0.04\n",
      "ROCAUC: 0.92 +- 0.03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt1 = DecisionTreeClassifier()\n",
    "validate(dt1)\n",
    "test(dt1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c82775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>test2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test  test2\n",
       "0     1      4\n",
       "1     2      5\n",
       "2     3      6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({\"test\": [1,2,3], \"test2\": [4,5,6]})\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb81442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.289648506151142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(np.mean(df[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
